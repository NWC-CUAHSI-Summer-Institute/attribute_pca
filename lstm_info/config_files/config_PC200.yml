# --- Experiment configurations --------------------------------------------------------------------

# experiment name, used as folder name
experiment_name: PC200_seed5

# files to specify training, validation and test basins (relative to code root or absolute path)
train_basin_file: ./basin_lists/list_389_camels_basins_random.txt
validation_basin_file: ./basin_lists/list_389_camels_basins_random.txt
test_basin_file: ./basin_lists/list_131_camels_basins_random.txt

# training, validation and test time periods (format = 'dd/mm/yyyy')
train_start_date: "01/10/1989"
train_end_date: "30/09/2008"
validation_start_date: "01/10/1983"
validation_end_date: "30/09/1985"
test_start_date: "01/10/2009"
test_end_date: "30/09/2014"

seed: 5

# which GPU (id) to use [in format of cuda:0, cuda:1 etc, or cpu or None]
device: cuda:0

# --- Validation configuration ---------------------------------------------------------------------

# specify after how many epochs to perform validation
validate_every: 100

# specify how many random basins to use for validation
validate_n_random_basins: 1

# specify which metrics to calculate during validation (see neuralhydrology.evaluation.metrics)
# this can either be a list or a dictionary. If a dictionary is used, the inner keys must match the name of the
# target_variable specified below. Using dicts allows for different metrics per target variable.
metrics:
  - NSE
  
allow_subsequent_nan_losses: 150

# --- Model configuration --------------------------------------------------------------------------

# base model type [lstm, ealstm, cudalstm, embcudalstm, mtslstm]
# (has to match the if statement in modelzoo/__init__.py)
model: cudalstm

# prediction head [regression]. Define the head specific parameters below
head: regression

# ----> Regression settings <----
output_activation: linear

# ----> General settings <----

# Number of cell states of the LSTM
hidden_size: 64

# Initial bias value of the forget gate
initial_forget_bias: 3

# Dropout applied to the output of the LSTM
output_dropout: 0.4

# --- Training configuration -----------------------------------------------------------------------

# specify optimizer [Adam]
optimizer: Adam

# specify loss [MSE, NSE, RMSE]
loss: NSE

# specify learning rates to use starting at specific epochs (0 is the initial learning rate)
learning_rate:
  0: 1e-2
  2: 1e-3
  4: 1e-4
  6: 1e-5
  8: 1e-6

# Mini-batch size
batch_size: 256

# Number of training epochs
epochs: 2

# If a value, clips the gradients during training to that norm.
clip_gradient_norm: 1

# Defines which time steps are used to calculate the loss. Can't be larger than seq_length.
# If use_frequencies is used, this needs to be a dict mapping each frequency to a predict_last_n-value, else an int.
predict_last_n: 1

# Length of the input sequence
# If use_frequencies is used, this needs to be a dict mapping each frequency to a seq_length, else an int.
seq_length: 365

# Number of parallel workers used in the data pipeline
num_workers: 8

# Log the training loss every n steps
log_interval: 5

# If true, writes logging results into tensorboard file
log_tensorboard: False

# If a value and greater than 0, logs n random basins as figures during validation
log_n_figures: 10

# Save model weights every n epochs
save_weights_every: 1

# --- Data configurations --------------------------------------------------------------------------

# which data set to use [camels_us, camels_gb, global, hourly_camels_us]
dataset: camels_us

# Path to data set root
data_dir: ../data/CAMELS_US

# Forcing product [daymet, maurer, maurer_extended, nldas, nldas_extended, nldas_hourly]
# can be either a list of forcings or a single forcing product
forcings:
  - nldas

dynamic_inputs:
  - PRCP(mm/day)
  - SRAD(W/m2)
  - Tmax(C)
  - Tmin(C)
  - Vp(Pa)

static_attributes:
  - PC1
  - PC2
  - PC3
  - PC4
  - PC5
  - PC6
  - PC7
  - PC8
  - PC9
  - PC10
  - PC11
  - PC12
  - PC13
  - PC14
  - PC15
  - PC16
  - PC17
  - PC18
  - PC19
  - PC20
  - PC21
  - PC22
  - PC23
  - PC24
  - PC25
  - PC26
  - PC27
  - PC28
  - PC29
  - PC30
  - PC31
  - PC32
  - PC33
  - PC34
  - PC35
  - PC36
  - PC37
  - PC38
  - PC39
  - PC40
  - PC41
  - PC42
  - PC43
  - PC44
  - PC45
  - PC46
  - PC47
  - PC48
  - PC49
  - PC50
  - PC51
  - PC52
  - PC53
  - PC54
  - PC55
  - PC56
  - PC57
  - PC58
  - PC59
  - PC60
  - PC61
  - PC62
  - PC63
  - PC64
  - PC65
  - PC66
  - PC67
  - PC68
  - PC69
  - PC70
  - PC71
  - PC72
  - PC73
  - PC74
  - PC75
  - PC76
  - PC77
  - PC78
  - PC79
  - PC80
  - PC81
  - PC82
  - PC83
  - PC84
  - PC85
  - PC86
  - PC87
  - PC88
  - PC89
  - PC90
  - PC91
  - PC92
  - PC93
  - PC94
  - PC95
  - PC96
  - PC97
  - PC98
  - PC99
  - PC100
  - PC101
  - PC102
  - PC103
  - PC104
  - PC105
  - PC106
  - PC107
  - PC108
  - PC109
  - PC110
  - PC111
  - PC112
  - PC113
  - PC114
  - PC115
  - PC116
  - PC117
  - PC118
  - PC119
  - PC120
  - PC121
  - PC122
  - PC123
  - PC124
  - PC125
  - PC126
  - PC127
  - PC128
  - PC129
  - PC130
  - PC131
  - PC132
  - PC133
  - PC134
  - PC135
  - PC136
  - PC137
  - PC138
  - PC139
  - PC140
  - PC141
  - PC142
  - PC143
  - PC144
  - PC145
  - PC146
  - PC147
  - PC148
  - PC149
  - PC150
  - PC151
  - PC152
  - PC153
  - PC154
  - PC155
  - PC156
  - PC157
  - PC158
  - PC159
  - PC160
  - PC161
  - PC162
  - PC163
  - PC164
  - PC165
  - PC166
  - PC167
  - PC168
  - PC169
  - PC170
  - PC171
  - PC172
  - PC173
  - PC174
  - PC175
  - PC176
  - PC177
  - PC178
  - PC179
  - PC180
  - PC181
  - PC182
  - PC183
  - PC184
  - PC185
  - PC186
  - PC187
  - PC188
  - PC189
  - PC190
  - PC191
  - PC192
  - PC193
  - PC194
  - PC195
  - PC196
  - PC197
  - PC198
  - PC199
  - PC200

# which columns to use as target
target_variables:
  - QObs(mm/d)

# clip negative predictions to zero for all variables listed below. Should be a list, even for single variables.
clip_targets_to_zero:
  - QObs(mm/d)
