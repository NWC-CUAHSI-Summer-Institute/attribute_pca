# --- Experiment configurations --------------------------------------------------------------------

# experiment name, used as folder name
experiment_name: HA_seed6

# files to specify training, validation and test basins (relative to code root or absolute path)
train_basin_file: ./basin_lists/list_389_camels_basins_random.txt
validation_basin_file: ./basin_lists/list_389_camels_basins_random.txt
test_basin_file: ./basin_lists/list_131_camels_basins_random.txt

# training, validation and test time periods (format = 'dd/mm/yyyy')
train_start_date: "01/10/1989"
train_end_date: "30/09/2008"
validation_start_date: "01/10/1983"
validation_end_date: "30/09/1985"
test_start_date: "01/10/2009"
test_end_date: "30/09/2014"

seed: 6

# which GPU (id) to use [in format of cuda:0, cuda:1 etc, or cpu or None]
device: cuda:0

# --- Validation configuration ---------------------------------------------------------------------

# specify after how many epochs to perform validation
validate_every: 100

# specify how many random basins to use for validation
validate_n_random_basins: 1

# specify which metrics to calculate during validation (see neuralhydrology.evaluation.metrics)
# this can either be a list or a dictionary. If a dictionary is used, the inner keys must match the name of the
# target_variable specified below. Using dicts allows for different metrics per target variable.
metrics:
  - NSE
  
allow_subsequent_nan_losses: 150

# --- Model configuration --------------------------------------------------------------------------

# base model type [lstm, ealstm, cudalstm, embcudalstm, mtslstm]
# (has to match the if statement in modelzoo/__init__.py)
model: cudalstm

# prediction head [regression]. Define the head specific parameters below
head: regression

# ----> Regression settings <----
output_activation: linear

# ----> General settings <----

# Number of cell states of the LSTM
hidden_size: 64

# Initial bias value of the forget gate
initial_forget_bias: 3

# Dropout applied to the output of the LSTM
output_dropout: 0.4

# --- Training configuration -----------------------------------------------------------------------

# specify optimizer [Adam]
optimizer: Adam

# specify loss [MSE, NSE, RMSE]
loss: NSE

# specify learning rates to use starting at specific epochs (0 is the initial learning rate)
learning_rate:
  0: 1e-2
  2: 1e-3
  4: 1e-4
  6: 1e-5
  8: 1e-6

# Mini-batch size
batch_size: 256

# Number of training epochs
epochs: 2

# If a value, clips the gradients during training to that norm.
clip_gradient_norm: 1

# Defines which time steps are used to calculate the loss. Can't be larger than seq_length.
# If use_frequencies is used, this needs to be a dict mapping each frequency to a predict_last_n-value, else an int.
predict_last_n: 1

# Length of the input sequence
# If use_frequencies is used, this needs to be a dict mapping each frequency to a seq_length, else an int.
seq_length: 365

# Number of parallel workers used in the data pipeline
num_workers: 8

# Log the training loss every n steps
log_interval: 5

# If true, writes logging results into tensorboard file
log_tensorboard: False

# If a value and greater than 0, logs n random basins as figures during validation
log_n_figures: 10

# Save model weights every n epochs
save_weights_every: 1

# --- Data configurations --------------------------------------------------------------------------

# which data set to use [camels_us, camels_gb, global, hourly_camels_us]
dataset: camels_us

# Path to data set root
data_dir: ../data/CAMELS_US

# Forcing product [daymet, maurer, maurer_extended, nldas, nldas_extended, nldas_hourly]
# can be either a list of forcings or a single forcing product
forcings:
  - nldas

dynamic_inputs:
  - PRCP(mm/day)
  - SRAD(W/m2)
  - Tmax(C)
  - Tmin(C)
  - Vp(Pa)

static_attributes:
  - aet_mm_s01
  - aet_mm_s02
  - aet_mm_s03
  - aet_mm_s04
  - aet_mm_s05
  - aet_mm_s06
  - aet_mm_s07
  - aet_mm_s08
  - aet_mm_s09
  - aet_mm_s10
  - aet_mm_s11
  - aet_mm_s12
  - aet_mm_syr
  - aet_mm_uyr
  - ari_ix_sav
  - ari_ix_uav
  - cls_cl_smj
  - cly_pc_sav
  - cly_pc_uav
  - clz_cl_smj
  - cmi_ix_s01
  - cmi_ix_s02
  - cmi_ix_s03
  - cmi_ix_s04
  - cmi_ix_s05
  - cmi_ix_s06
  - cmi_ix_s07
  - cmi_ix_s08
  - cmi_ix_s09
  - cmi_ix_s10
  - cmi_ix_s11
  - cmi_ix_s12
  - cmi_ix_syr
  - cmi_ix_uyr
  - crp_pc_sse
  - crp_pc_use
  - dis_m3_pmn
  - dis_m3_pmx
  - dis_m3_pyr
  - dor_pc_pva
  - ele_mt_sav
  - ele_mt_smn
  - ele_mt_smx
  - ele_mt_uav
  - ero_kh_sav
  - ero_kh_uav
  - fec_cl_smj
  - fmh_cl_smj
  - for_pc_sse
  - for_pc_use
  - gdp_ud_sav
  - gdp_ud_ssu
  - gdp_ud_usu
  - gla_pc_sse
  - gla_pc_use
  - glc_cl_smj
  - glc_pc_s01
  - glc_pc_s02
  - glc_pc_s04
  - glc_pc_s06
  - glc_pc_s09
  - glc_pc_s11
  - glc_pc_s12
  - glc_pc_s13
  - glc_pc_s14
  - glc_pc_s15
  - glc_pc_s16
  - glc_pc_s18
  - glc_pc_s20
  - glc_pc_s21
  - glc_pc_s22
  - glc_pc_u01
  - glc_pc_u02
  - glc_pc_u04
  - glc_pc_u06
  - glc_pc_u09
  - glc_pc_u11
  - glc_pc_u12
  - glc_pc_u13
  - glc_pc_u14
  - glc_pc_u15
  - glc_pc_u16
  - glc_pc_u18
  - glc_pc_u20
  - glc_pc_u21
  - glc_pc_u22
  - gwt_cm_sav
  - hdi_ix_sav
  - hft_ix_s09
  - hft_ix_s93
  - hft_ix_u09
  - hft_ix_u93
  - inu_pc_slt
  - inu_pc_smn
  - inu_pc_smx
  - inu_pc_ult
  - inu_pc_umn
  - inu_pc_umx
  - ire_pc_sse
  - ire_pc_use
  - kar_pc_sse
  - kar_pc_use
  - lit_cl_smj
  - lka_pc_sse
  - lka_pc_use
  - lkv_mc_usu
  - nli_ix_sav
  - nli_ix_uav
  - pac_pc_sse
  - pac_pc_use
  - pet_mm_s01
  - pet_mm_s02
  - pet_mm_s03
  - pet_mm_s04
  - pet_mm_s05
  - pet_mm_s06
  - pet_mm_s07
  - pet_mm_s08
  - pet_mm_s09
  - pet_mm_s10
  - pet_mm_s11
  - pet_mm_s12
  - pet_mm_syr
  - pet_mm_uyr
  - pnv_cl_smj
  - pnv_pc_s04
  - pnv_pc_s05
  - pnv_pc_s06
  - pnv_pc_s07
  - pnv_pc_s08
  - pnv_pc_s09
  - pnv_pc_s10
  - pnv_pc_s11
  - pnv_pc_s12
  - pnv_pc_s13
  - pnv_pc_u04
  - pnv_pc_u05
  - pnv_pc_u06
  - pnv_pc_u07
  - pnv_pc_u08
  - pnv_pc_u09
  - pnv_pc_u10
  - pnv_pc_u11
  - pnv_pc_u12
  - pnv_pc_u13
  - pop_ct_ssu
  - pop_ct_usu
  - ppd_pk_sav
  - ppd_pk_uav
  - pre_mm_s01
  - pre_mm_s02
  - pre_mm_s03
  - pre_mm_s04
  - pre_mm_s05
  - pre_mm_s06
  - pre_mm_s07
  - pre_mm_s08
  - pre_mm_s09
  - pre_mm_s10
  - pre_mm_s11
  - pre_mm_s12
  - pre_mm_syr
  - pre_mm_uyr
  - prm_pc_sse
  - prm_pc_use
  - pst_pc_sse
  - pst_pc_use
  - rdd_mk_sav
  - rdd_mk_uav
  - rev_mc_usu
  - ria_ha_ssu
  - ria_ha_usu
  - riv_tc_ssu
  - riv_tc_usu
  - run_mm_syr
  - sgr_dk_sav
  - slp_dg_sav
  - slp_dg_uav
  - slt_pc_sav
  - slt_pc_uav
  - snd_pc_sav
  - snd_pc_uav
  - snw_pc_s01
  - snw_pc_s02
  - snw_pc_s03
  - snw_pc_s04
  - snw_pc_s05
  - snw_pc_s06
  - snw_pc_s07
  - snw_pc_s08
  - snw_pc_s09
  - snw_pc_s10
  - snw_pc_s11
  - snw_pc_s12
  - snw_pc_smx
  - snw_pc_syr
  - snw_pc_uyr
  - soc_th_sav
  - soc_th_uav
  - swc_pc_s01
  - swc_pc_s02
  - swc_pc_s03
  - swc_pc_s04
  - swc_pc_s05
  - swc_pc_s06
  - swc_pc_s07
  - swc_pc_s08
  - swc_pc_s09
  - swc_pc_s10
  - swc_pc_s11
  - swc_pc_s12
  - swc_pc_syr
  - swc_pc_uyr
  - tbi_cl_smj
  - tec_cl_smj
  - tmp_dc_s01
  - tmp_dc_s02
  - tmp_dc_s03
  - tmp_dc_s04
  - tmp_dc_s05
  - tmp_dc_s06
  - tmp_dc_s07
  - tmp_dc_s08
  - tmp_dc_s09
  - tmp_dc_s10
  - tmp_dc_s11
  - tmp_dc_s12
  - tmp_dc_smn
  - tmp_dc_smx
  - tmp_dc_syr
  - tmp_dc_uyr
  - urb_pc_sse
  - urb_pc_use
  - wet_cl_smj
  - wet_pc_s01
  - wet_pc_s02
  - wet_pc_s03
  - wet_pc_s09
  - wet_pc_sg1
  - wet_pc_sg2
  - wet_pc_u01
  - wet_pc_u02
  - wet_pc_u03
  - wet_pc_u09
  - wet_pc_ug1
  - wet_pc_ug2

# which columns to use as target
target_variables:
  - QObs(mm/d)

# clip negative predictions to zero for all variables listed below. Should be a list, even for single variables.
clip_targets_to_zero:
  - QObs(mm/d)
